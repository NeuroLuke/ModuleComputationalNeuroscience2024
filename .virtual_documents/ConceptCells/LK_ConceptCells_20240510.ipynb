











# let's import some useful packages
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import mannwhitneyu # to compute Mann Whitney U tests
from scipy.ndimage import uniform_filter1d # for smoothing





# code for simulating the presentation of concepts





# code for simulating the activity of the concept cell


# code for plotting the simulated concept cell





# code for analyzing the concept cell








# settings for behavior
num_concepts = 20 # number of concepts
concepts = np.arange(0, num_concepts)
concept_per_trial = np.repeat(concepts, 20)
trial_idx = np.arange(0, len(concept_per_trial))

# in a real study, we would randomize the concepts across trials, but here we'll keep the block design for simplicity
# np.random.shuffle(concept_per_trial)

# settings for the firing rates of the cells
preferred_concept = 0 # preferred concept of the cell
preferred_latency = 0.4 # seconds; cf. Mormann et al., J Neurosci, 2008
preferred_firing_rate = 100 # Hz
baseline_firing_rate = 2 # Hz
preferred_width = 0.1 # standard deviation of the Gaussian distribution

# settings for the trials
trial_start_time = -1
trial_end_time = 1
trial_duration = trial_end_time - trial_start_time

# settinsg for the analysis
analysis_period_start = -1 # seconds
analysis_period_end = 1 # seconds
analysis_bin_size = 0.05 # Duration of each bin in milliseconds
analysis_num_bins = int((analysis_period_end - analysis_period_start) / analysis_bin_size) # number of bins
analysis_overlap = 0 # temporal overlap between bins

# individual analysis periods
analysis_periods = []
for i in range(analysis_num_bins):
    start = np.round(analysis_period_start + i * (analysis_bin_size - analysis_overlap), 3)
    end = np.round(start + analysis_bin_size, 3)
    analysis_periods.append([start, end])
analysis_time_points = np.mean(analysis_periods, axis = 1)

# settings for estimating statistics
stats_baseline_start = -1
stats_baseline_end = 0
stats_period_start = 0
stats_period_end = 1
stats_bin_size = 0.05
stats_num_bins = int((stats_period_end - stats_period_start) / stats_bin_size)
stats_overlap = 0

# individual stats periods
stats_periods = []
for i in range(stats_num_bins):
    start = np.round(stats_period_start + i * (stats_bin_size - stats_overlap), 3)
    end = np.round(start + stats_bin_size, 3)
    stats_periods.append([start, end])
stats_time_points = np.mean(stats_periods, axis = 1)

# settings for creating a raster plot
raster_period_start = -1
raster_period_end = 1
raster_bin_size = 0.01
raster_num_bins = int((raster_period_end - raster_period_start) / raster_bin_size)
raster_overlap = 0

# individual raster periods
raster_periods = []
for i in range(raster_num_bins):
    start = np.round(raster_period_start + i * (raster_bin_size - raster_overlap), 3)
    end = np.round(start + raster_bin_size, 3)
    raster_periods.append([start, end])
raster_time_points = np.mean(raster_periods, axis = 1)








# preallocate spike times for each trial
spike_times = [np.nan] * len(concept_per_trial)

# loop through trials
for i_trial in range(len(concept_per_trial)):

    # baseline spike times (uniformly distributed)
    baseline_num_spikes = trial_duration * baseline_firing_rate
    baseline_spike_times = np.random.uniform(low = trial_start_time, high = trial_end_time, size = int(baseline_num_spikes))
    baseline_spike_times.sort()

    # add new spike times
    spike_times[i_trial] = baseline_spike_times

    # if it is the cell's preferred concept
    if concept_per_trial[i_trial] == preferred_concept:

        # calculate expected number of peak spikes
        peak_num_spikes = (preferred_firing_rate - baseline_firing_rate) * preferred_width * np.sqrt(2 * np.pi) # approximation based on area under Gaussian curve

        # generate random spike times for peak activity (normally distributed)
        peak_spike_times = np.random.normal(loc = preferred_latency, scale = preferred_width, size = int(peak_num_spikes))

        # combine and sort spike times
        all_spike_times = np.concatenate((baseline_spike_times, peak_spike_times))
        all_spike_times.sort()

        # add new spikes
        spike_times[i_trial] = all_spike_times








# create a raster plot of the spike times
num_spikes_raster = np.full((len(concept_per_trial), len(raster_periods)), np.nan)
for i_trial in range(num_spikes_raster.shape[0]):
    for i_time in range(num_spikes_raster.shape[1]):
        num_spikes_raster[i_trial, i_time] = np.sum((spike_times[i_trial] >= raster_periods[i_time][0]) & (spike_times[i_trial] < raster_periods[i_time][1]))


# show the raster plot
plt.figure()
plt.imshow(num_spikes_raster > 0, aspect = 'auto', cmap = 'gray_r')
plt.xlabel('Time (seconds))')
plt.ylabel('Trials')
plt.xticks(ticks = np.linspace(0, num_spikes_raster.shape[1], 5), labels = np.round(np.linspace(np.min(raster_time_points), np.max(raster_time_points), 5), 4));








# estimate number of spikes in each trial * time-bin
num_spikes = np.full((len(concept_per_trial), len(analysis_periods)), np.nan)
num_spikes_baseline = np.full((len(concept_per_trial)), np.nan)
num_spikes_stats = np.full((len(concept_per_trial), len(stats_periods)), np.nan)
for i_trial in range(num_spikes.shape[0]):
    # number of spikes in each time bin
    for i_time in range(num_spikes.shape[1]):
        num_spikes[i_trial, i_time] = np.sum((spike_times[i_trial] >= analysis_periods[i_time][0]) & (spike_times[i_trial] < analysis_periods[i_time][1]))
    # number of spikes in each baseline period
    num_spikes_baseline[i_trial] = np.sum((spike_times[i_trial] >= stats_baseline_start) & (spike_times[i_trial] < stats_baseline_end))
    # number of spikes in each stats period
    for i_time in range(num_spikes_stats.shape[1]):
        num_spikes_stats[i_trial, i_time] = np.sum((spike_times[i_trial] >= stats_periods[i_time][0]) & (spike_times[i_trial] < stats_periods[i_time][1]))

# estimate firing rate in each trial * time-bin
FR = num_spikes / analysis_bin_size
FR_baseline = num_spikes_baseline / (stats_baseline_end - stats_baseline_start)
FR_stats = num_spikes_stats / stats_bin_size

# smooth the firing rates
FR = uniform_filter1d(FR, size = 5, axis = 1)
FR_stats = uniform_filter1d(FR_stats, size = 5, axis = 1)





# plotting the firing rates for the analysis
plt.figure()
plt.plot(analysis_time_points, np.mean(FR[concept_per_trial == preferred_concept, :], axis = 0))
plt.plot(analysis_time_points, np.mean(FR[concept_per_trial != preferred_concept, :], axis = 0))
plt.xlabel('Time (sec)')
plt.ylabel('Firing rate (Hz)')
plt.legend(['Preferred concept', 'Other concepts'])
plt.show()








# preallocate the p-values of the statistical tests
p = np.full((len(concepts), stats_num_bins), np.nan)

# loop through the concepts
for i_concept in range(len(concepts)):
    this_FR_baseline = FR_baseline[concept_per_trial == i_concept]
    this_FR_stats = FR_stats[concept_per_trial == i_concept]
    # loop through time points
    for i_time in range(this_FR_stats.shape[1]):
        _, p[i_concept, i_time] = mannwhitneyu(this_FR_stats[:, i_time], this_FR_baseline, alternative = 'greater')





# Bonferroni correction for the number of concepts and time points
p = p * np.size(p)





print('The minimum p-value (Bonferroni-corrected) is: ' + str(np.min(p)))









